<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>
      What Are AI Benchmarks? - AI Literacy for Students
    </title>
    <link href="https://fonts.googleapis.com" rel="preconnect">
    <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin="">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&amp;display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css" integrity="sha512-SnH5WK+bZxgPHs44uWIX+LLJAJ9/2PkPKZ5QiAj6Ta86w+fsb2TkcmfRyVX3pBnMFcV7oQPJkl9QevSCWr3W6A==" crossorigin="anonymous" referrerpolicy="no-referrer">
    <link rel="stylesheet" href="styles.css">
  </head>
  <body>
    <header class="site-header">
      <div class="site-header__inner">
        <a class="site-header__title" href="index.html">
          AI Literacy for Students
        </a>
        <p class="site-header__tagline">
          Understand, explore, and apply generative AI responsibly.
        </p>
      </div>
    </header>
    <nav class="site-nav" aria-label="Primary" data-nav-placeholder="">
      <div class="site-nav__loading">
        Loading navigation…
      </div>
      <noscript>
        <div class="site-nav__noscript">
          Navigation requires JavaScript to display. Enable scripts in your browser to use this menu.
        </div>
      </noscript>
    </nav>
    <div id="gai-wrap">
      <main>
        <div class="gai-cont">
          <section class="gai-box">
            <h1>
              What Are AI Benchmarks?
            </h1>
            <p>
              An AI benchmark is a standardized test for AI models—like an IQ test for artificial intelligence. You can literally give an IQ test to an AI model to see how it performs. GPT-4, for example, has been tested on various IQ-style assessments, but does that actually mean anything?
            </p>
            <p>
              Whenever a new AI model is released, companies tout their benchmark scores as proof of superiority. These scores provide useful comparisons, but they don't tell the whole story. Models often excel at narrow test scenarios while struggling with real-world applications—a perfect example of the Jagged Frontier in action.
            </p>
            <div>
              <h3>
                The Jagged Frontier Connection
              </h3>
              <p>
                Benchmarks reveal the "jaggedness" of AI capabilities. A model might ace graduate-level physics problems (GPQA benchmark) while failing at simple common-sense reasoning tasks. This unpredictability is why we can't rely on benchmarks alone.
              </p>
            </div>
          </section>
          <section class="gai-box">
            <h2>
              Example Benchmarks
            </h2>
            <p>
              Understanding benchmark categories helps you understand AI capabilities. Here are some examples of what different benchmarks measure:
            </p>
            <div>
              <details>
                <summary>
                  <h3>
                    General Knowledge & Academic Exams
                  </h3>
                </summary>
                <div>
                  <p>
                    <strong>
                      MMLU (Massive Multitask Language Understanding):
                    </strong>
                    Tests knowledge across 57 subjects from elementary to professional level through multiple-choice questions. Think of it as a comprehensive general knowledge exam that covers everything from basic math to advanced law.
                  </p>
                  <p>
                    <strong>
                      AGIEval:
                    </strong>
                    Uses real standardized exams like the SAT, LSAT, and GRE. This benchmark tells us how well AI performs on the same tests humans take for college and graduate school admissions.
                  </p>
                  <p>
                    <strong>
                      Other Benchmarks in this category:
                    </strong>
                    RACE (Reading Comprehension from Examinations), Humanities Last Exam (designed as a "final exam" before human-level intelligence)
                  </p>
                  <div>
                    <p>
                      <strong>
                        Why it matters:
                      </strong>
                      Shows breadth of knowledge but doesn't guarantee practical application or true understanding.
                    </p>
                  </div>
                </div>
              </details>
              <details>
                <summary>
                  <h3>
                    Reasoning & Truthfulness
                  </h3>
                </summary>
                <div>
                  <p>
                    <strong>
                      TruthfulQA:
                    </strong>
                    Contains 817 questions specifically designed to elicit common misconceptions and falsehoods. It tests whether AI will confidently state incorrect "facts" that sound plausible.
                  </p>
                  <p>
                    <strong>
                      HellaSwag:
                    </strong>
                    Evaluates common-sense reasoning about everyday scenarios through story completion tasks. Can the AI predict what happens next in ordinary situations?
                  </p>
                  <p>
                    <strong>
                      Other Benchmarks in this category:
                    </strong>
                    GPQA (Graduate-Level Google-Proof Q&A), BIG-bench (200+ diverse reasoning tasks), ARC (AI2 Reasoning Challenge for grade-school science)
                  </p>
                  <div>
                    <p>
                      <strong>
                        Why it matters:
                      </strong>
                      Reveals tendency to hallucinate or perpetuate misinformation, critical for trust and reliability.
                    </p>
                  </div>
                </div>
              </details>
              <details>
                <summary>
                  <h3>
                    Software Engineering & Code
                  </h3>
                </summary>
                <div>
                  <p>
                    <strong>
                      HumanEval:
                    </strong>
                    The gold standard with 164 programming problems. It's become the industry baseline for measuring code generation capabilities.
                  </p>
                  <p>
                    <strong>
                      SWE-bench:
                    </strong>
                    Tests real-world programming by having AI solve actual GitHub issues. The model must understand the codebase, identify the problem, and write a working patch.
                  </p>
                  <p>
                    <strong>
                      Other Benchmarks in this category:
                    </strong>
                    BigCodeBench (more challenging than HumanEval, better at distinguishing top-tier models)
                  </p>
                  <div>
                    <p>
                      <strong>
                        Why it matters:
                      </strong>
                      Measures practical coding ability, not just syntax knowledge—can it actually help you program?
                    </p>
                  </div>
                </div>
              </details>
              <details>
                <summary>
                  <h3>
                    Conversational Quality
                  </h3>
                </summary>
                <div>
                  <p>
                    <strong>
                      MT-Bench:
                    </strong>
                    Evaluates multi-turn dialogue across reasoning, math, coding, and roleplay. Judges score paired outputs to determine which is better.
                  </p>
                  <p>
                    <strong>
                      Chatbot Arena (LMArena):
                    </strong>
                    Real users vote on which model gives better responses in head-to-head comparisons, without knowing which model is which.
                  </p>
                  <div>
                    <p>
                      <strong>
                        Why it matters:
                      </strong>
                      Captures subjective quality that automated tests miss—the "feel" of talking to the AI.
                    </p>
                  </div>
                </div>
              </details>
              <details>
                <summary>
                  <h3>
                    Multimodal & Specialized
                  </h3>
                </summary>
                <div>
                  <p>
                    <strong>
                      MMMU (Massive Multi-discipline Multimodal Understanding):
                    </strong>
                    Extension of MMLU that includes images, diagrams, and charts. Critical for evaluating modern multimodal models.
                  </p>
                  <p>
                    <strong>
                      Other Benchmarks in this category:
                    </strong>
                    Kaggle Game Arena (strategic reasoning through game-playing), HELM (Holistic Evaluation across multiple metrics including fairness and bias)
                  </p>
                  <div>
                    <p>
                      <strong>
                        Why it matters:
                      </strong>
                      Tests whether AI can work with real-world content that combines text and visuals.
                    </p>
                  </div>
                </div>
              </details>
            </div>
            <div>
              <img alt="Example benchmark performance comparison across different AI models" loading="lazy" src="[Space for benchmark performance graph]">
              <p>
                Example benchmark performance graph comparing different models will be added here
              </p>
            </div>
          </section>
        </div>
        <section class="gai-box">
          <h2>
            Limitations of Benchmarks
          </h2>
          <div>
            <h3>
              Critical Limitations to Understand
            </h3>
            <div>
              <div>
                <h3>
                  Data Contamination
                </h3>
                <p>
                  Test questions leak into training data, turning reasoning tests into memory tests. Models aren't "thinking"—they're reciting memorized answers.
                </p>
              </div>
              <div>
                <h3>
                  Benchmark Gaming
                </h3>
                <p>
                  Models are increasingly optimized specifically to ace benchmarks, like "teaching to the test" in education. High scores don't guarantee real-world performance.
                </p>
              </div>
              <div>
                <h3>
                  Cultural & Linguistic Bias
                </h3>
                <p>
                  Most benchmarks reflect Western, English-centric perspectives, disadvantaging models trained on diverse data and limiting global applicability.
                </p>
              </div>
              <div>
                <h3>
                  Moving Goalposts
                </h3>
                <p>
                  As models improve, benchmarks become obsolete. What was "superhuman" last year is baseline today, making historical comparisons difficult.
                </p>
              </div>
            </div>
          </div>
        </section>
        <section class="gai-box">
          <h2>
            Beyond Traditional Benchmarks
          </h2>
          <p>
            To address these limitations, the field is evolving toward more robust evaluation methods:
          </p>
          <div>
            <div>
              <h3>
                Dynamic Arenas
              </h3>
              <p>
                Instead of fixed tests, models compete head-to-head. Users judge outputs without knowing which model produced what, eliminating brand bias.
              </p>
            </div>
            <div>
              <h3>
                Human-in-the-Loop
              </h3>
              <p>
                For subjective qualities like creativity or empathy, human experts grade performance in real-time, capturing nuances automated tests miss.
              </p>
            </div>
            <div>
              <h3>
                Adversarial Testing
              </h3>
              <p>
                Intentionally trying to break models reveals hidden weaknesses that standard benchmarks miss, improving safety and reliability.
              </p>
            </div>
          </div>
        </section>
        <div>
          <section class="gai-box">
            <h2>
              Activity: Create Your Personal AI Benchmark
            </h2>
            <div>
              <h3>
                Privacy Reminder
              </h3>
              <p>
                When creating personal benchmarks, always use:
              </p>
              <ul>
                <li>
                  Hypothetical scenarios instead of real personal data
                </li>
                <li>
                  Public information rather than confidential details
                </li>
                <li>
                  Generic examples that mirror your needs without exposing sensitive information
                </li>
              </ul>
            </div>
            <p>
              The best measure of an AI isn't a generic score—it's how well it helps YOU achieve YOUR specific goals. Let's build your personal benchmark.
            </p>
            <div>
              <details>
                <summary>
                  <p>
                    Approach 1: Quick Evaluation Framework
                  </p>
                </summary>
                <div>
                  <h3>
                    Task
                  </h3>
                  <p>
                    Select 3-5 challenges from the "Exploring the Jagged Frontier" module that matter to your work.
                  </p>
                  <h3>
                    Dive-in & Do
                  </h3>
                  <ol>
                    <li>
                      Test each challenge on 2-3 different AI models
                    </li>
                    <li>
                      Document results in a simple spreadsheet
                    </li>
                    <li>
                      Re-test quarterly as new models emerge
                    </li>
                  </ol>
                  <h3>
                    Pause-and-Ponder
                  </h3>
                  <p>
                    Which failures surprised you most? What does this reveal about the model's training?
                  </p>
                </div>
              </details>
              <details>
                <summary>
                  <p>
                    Approach 2: Comprehensive Personal Benchmark
                  </p>
                </summary>
                <div>
                  <div>
                    <h3>
                      Task
                    </h3>
                    <p>
                      Design a multi-faceted evaluation suite tailored to your professional needs.
                    </p>
                    <h3>
                      Dive-in & Do
                    </h3>
                    <p>
                      Choose 2-3 tasks from this list (or create your own):
                    </p>
                  </div>
                  <div>
                    <div>
                      <h3>
                        Career Agent
                      </h3>
                      <p>
                        Provide career interests and ask the AI to identify suitable companies and draft outreach messages.
                      </p>
                    </div>
                    <div>
                      <h3>
                        Marketing Strategist
                      </h3>
                      <p>
                        Give a product and target audience. Request a 3-month campaign with content calendar and materials.
                      </p>
                    </div>
                    <div>
                      <h3>
                        Event Planner
                      </h3>
                      <p>
                        Describe an event. Request complete project plan with timeline, budget, and promotional materials.
                      </p>
                    </div>
                    <div>
                      <h3>
                        Travel Planner
                      </h3>
                      <p>
                        Specify destination, duration, budget, and interests. Request day-by-day itinerary with logistics.
                      </p>
                    </div>
                    <div>
                      <h3>
                        DIY Guide
                      </h3>
                      <p>
                        Describe a DIY project. Request step-by-step instructions, materials list, and video script.
                      </p>
                    </div>
                    <div>
                      <h3>
                        Dungeon Master
                      </h3>
                      <p>
                        Provide a fantasy setting. Test the AI's ability to maintain narrative consistency and adapt.
                      </p>
                    </div>
                  </div>
                  <div>
                    <h3>
                      Evaluation Criteria
                    </h3>
                    <ol>
                      <li>
                        <strong>
                          Accuracy:
                        </strong>
                        Are facts correct? Do recommendations make sense?
                      </li>
                      <li>
                        <strong>
                          Completeness:
                        </strong>
                        Did it address all requirements?
                      </li>
                      <li>
                        <strong>
                          Creativity:
                        </strong>
                        Are suggestions original or clichéd?
                      </li>
                      <li>
                        <strong>
                          Practicality:
                        </strong>
                        Could you actually implement the output?
                      </li>
                      <li>
                        <strong>
                          Time Saved:
                        </strong>
                        How much faster than doing it yourself?
                      </li>
                    </ol>
                    <h3>
                      Pause-and-Ponder
                    </h3>
                    <p>
                      What patterns emerge across different tasks? Where does the AI consistently excel or fail? How might this shape your workflow integration?
                    </p>
                  </div>
                </div>
              </details>
            </div>
          </section>
          <div>
            <h3>
              For Educators
            </h3>
            <p>
              Consider having students create a class benchmark suite that tests AI capabilities relevant to your discipline. This exercise builds critical evaluation skills while revealing the Jagged Frontier in action.
            </p>
          </div>
        </div>
        <div>
          <h3>
            Key Takeaways
          </h3>
          <ul>
            <li>
              Benchmarks provide useful comparisons but don't capture real-world performance
            </li>
            <li>
              Data contamination and benchmark gaming inflate scores artificially
            </li>
            <li>
              The "Jagged Frontier" means excellence in benchmarks doesn't guarantee practical utility
            </li>
            <li>
              Personal benchmarks tailored to your needs are more valuable than generic scores
            </li>
            <li>
              Regular re-evaluation helps you stay current with rapidly evolving capabilities
            </li>
          </ul>
        </div>
        <hr>
        <div>
          <h3>
            Further Reading
          </h3>
          <p>
            <a href="https://arxiv.org/abs/2311.12022" rel="noopener noreferrer" target="_blank">
              Beyond Benchmarks: Rethinking Evaluation in AI
            </a>
            - Technical deep-dive into benchmark limitations
          </p>
          <p>
            <a href="https://www.anthropic.com/index/evaluating-ai-systems" rel="noopener noreferrer" target="_blank">
              Anthropic's Approach to AI Evaluation
            </a>
            - Industry perspective on comprehensive testing
          </p>
          <p>
            <a href="#" rel="noopener noreferrer" target="_blank">
              Return to Exploring the Jagged Frontier
            </a>
            - See how benchmarks connect to capability mapping
          </p>
        </div>
        <footer>
          <p class="gai-end-txt">
            Unless otherwise stated, this page and
            <a href="index.html">
              AI Literacy for Students
            </a>
            © 2025 by David Williams is licensed under
            <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">
              Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International
            </a>
            <span class="gai-cc-icons">
              <img loading="lazy" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg" alt="Creative Commons icon">
              <img loading="lazy" src="https://mirrors.creativecommons.org/presskit/icons/by.svg" alt="Attribution icon">
              <img loading="lazy" src="https://mirrors.creativecommons.org/presskit/icons/nc.svg" alt="Non-Commercial icon">
              <img loading="lazy" src="https://mirrors.creativecommons.org/presskit/icons/sa.svg" alt="Share Alike icon">
            </span>
          </p>
        </footer>
      </main>
    </div>
    <script src="nav.js"></script>
  </body>
</html>
