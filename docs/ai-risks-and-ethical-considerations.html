<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>AI Risks &amp; Ethical Considerations - AI Literacy for Students</title>
    <link href="https://fonts.googleapis.com" rel="preconnect" />
    <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin />
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css" integrity="sha512-SnH5WK+bZxgPHs44uWIX+LLJAJ9/2PkPKZ5QiAj6Ta86w+fsb2TkcmfRyVX3pBnMFcV7oQPJkl9QevSCWr3W6A==" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <link rel="stylesheet" href="styles.css" />
</head>
<body>
<header class="site-header">
    <div class="site-header__inner">
        <a class="site-header__title" href="index.html">AI Literacy for Students</a>
        <p class="site-header__tagline">Understand, explore, and apply generative AI responsibly.</p>
    </div>
</header>
<nav class="site-nav" aria-label="Primary">
    <ul class="site-nav__list">
        <li class='site-nav__item'>
            <a href='index.html'>Pillars of AI Literacy</a>
        </li>
        <li class='site-nav__item site-nav__item--has-children'>
            <a href='authentic-learning-and-ai-use.html'>Authentic Learning and AI Use</a>
            <ul class='site-nav__sublist'>
                <li class='site-nav__subitem'><a href='learning-is-hard-and-its-supposed-to-be.html'>Learning is Hard - and it's supposed to be</a></li>
            </ul>
        </li>
        <li class='site-nav__item site-nav__item--has-children'>
            <a href='understand-and-explore-generative-ai.html'>Understand and Explore Generative AI</a>
            <ul class='site-nav__sublist'>
                <li class='site-nav__subitem'><a href='gen-ai-fundamentals.html'>Gen AI Fundamentals</a></li>
                <li class='site-nav__subitem'><a href='gen-ai-behind-the-curtain.html'>Gen AI - Behind the Curtain</a></li>
                <li class='site-nav__subitem'><a href='gen-ai-tools-platforms-and-interfaces.html'>Gen AI Tools, Platforms, and Interfaces</a></li>
                <li class='site-nav__subitem'><a href='potential-and-limitations-of-gen-ai.html'>Potential and Limitations of Gen AI</a></li>
            </ul>
        </li>
        <li class='site-nav__item site-nav__item--has-children site-nav__item--active'>
            <a href='analyze-and-apply-gen-ai.html'>Analyze and Apply Gen AI</a>
            <ul class='site-nav__sublist'>
                <li class='site-nav__subitem'><a href='essentials-for-smart-engagement.html'>Essentials for Smart Engagement</a></li>
                <li class='site-nav__subitem'><a href='ai-quick-queries-and-idea-sparker.html'>AI Quick Queries & Idea Sparker</a></li>
                <li class='site-nav__subitem'><a href='ai-study-buddy-and-skill-builder.html'>AI Study Buddy & Skill Builder</a></li>
                <li class='site-nav__subitem'><a href='ai-generated-writing-effective-and-ethical-use.html'>AI-Generated Writing: Effective and Ethical Use</a></li>
                <li class='site-nav__subitem'><a href='ai-research-assistants.html'>AI Research Assistants</a></li>
                <li class='site-nav__subitem site-nav__subitem--active'><a href='ai-risks-and-ethical-considerations.html'>AI Risks & Ethical Considerations</a></li>
                <li class='site-nav__subitem'><a href='ais-jagged-frontier.html'>AI's Jagged Frontier</a></li>
            </ul>
        </li>
        <li class='site-nav__item'>
            <a href='ai-contribution-statement.html'>AI Contribution Statement</a>
        </li>
    </ul>
</nav>
<div id="gai-wrap">
    <main>
        <div class="gai-cont">
<section class="gai-box">
            <h2>AI Risks &amp; Ethical Considerations</h2>
            <div>

            <div>
            <h1>AI Risks and Ethical Impacts</h1>

            <div>
        </section>
        <section class="gai-box">
            <h2><i></i>Introduction: A Structured Lens</h2>
            <p>To use AI responsibly, you need to see understand its potential and its risks. The examples below will help you see some of the biggest risks and ethical dilemmas presented by AI. We will look at each example through a simple framework of its</p>

            <ul>
            	<li>severity: how bad is the impact</li>
            	<li> exposure: who or how much of the population is at risk</li>
            	<li>plausibility: how likely that we will see any impacts</li>
            	<li>prevalence: how often we are seeing impacts.</li>
            </ul>

            <p>Please note, that this is not a comprehensive list. If you have ideas of things that are missed, please reach out to the author.</p>

            <p>After working through these examples, hopefully you will see the interconnectivity of the risks and have a bit of an understanding of what you can do and what needs to be done to mitigate them.</p>
            </div>

            <div>
            <details><summary> <i></i> Information Integrity &amp; Civic Harms </summary>

            <div>
            <div><img alt="Abstract digital art showing a network of nodes being fractured by chaotic red lines, representing misinformation." loading="lazy" src="https://d1qywhc7l90rsa.cloudfront.net/accounts/165495/images/Info_Integrity_and_Civic_Harms.png"></div>

            <p>AI makes it easy to create and spread fake content. This can be anything from deepfakes to targeted propaganda. The result is a loss of trust, confusion, and rising social tension.</p>

            <p><strong>Example:</strong> A doctored video of a local candidate circulates before an election, changing public opinion before it can be debunked.</p>


            <table>
            	<thead>
            		<tr>
            			<th>Factor</th>
            			<th>Assessment</th>
            		</tr>
            	</thead>
            	<tbody>
            		<tr>
            			<td><span><i></i><strong>Severity</strong></span></td>
            			<td>High. Harms can impact public safety, elections, and trust in institutions.</td>
            		</tr>
            		<tr>
            			<td><span><i></i><strong>Exposure</strong></span></td>
            			<td>Broad. Content can reach huge audiences very quickly.</td>
            		</tr>
            		<tr>
            			<td><span><i></i><strong>Plausibility</strong></span></td>
            			<td>High. Tools for creating synthetic media are widely available.</td>
            		</tr>
            		<tr>
            			<td><span><i></i><strong>Prevalence</strong></span></td>
            			<td>Increasing. Misinformation campaigns appear in every election cycle.</td>
            		</tr>
            		<tr>
            			<td><span><i></i><strong>Mitigation</strong></span></td>
            			<td>
            			<p><strong>Systemic &amp; Institutional:</strong> Platforms can help by adding content labels, implementing strong guardrails, and using rapid fact-checking.</p>

            			<p><strong>Individual &amp; Classroom Actions:</strong></p>

            			<ul>
            				<li>Read laterally. When you see a claim, open new tabs to check it against other reliable sources.</li>
            				<li>Check the source. Use reverse image search to find a photo&#x27;s origin.</li>
            				<li>Pause before you post. Always verify information before you share it, especially if it makes you feel a strong emotion.</li>
            			</ul>
            			</td>
            		</tr>
            	</tbody>
            </table>

            <div>
            <h4><i></i>Quick Activity</h4>

            <p>Pick a trending video or post. Trace its source, check fact-checks, and look for content credentials. Discuss how one small edit could flip its meaning.</p>
            </div>

            <div>
            <h5><i></i>Dive Deeper</h5>

            <p>1) <a href="https://www.cisa.gov/sites/default/files/2024-10/PSA_Just_So_You_Know_Foreign_Threat_Actors_Likely_to_Use_a_Variety_of_TacticsV2-508.pdf" rel="noopener" target="_blank">CISA, Deepfakes &amp; Synthetic Media</a><br>
            2) <a href="https://syntheticmedia.partnershiponai.org/" rel="noopener" target="_blank">Partnership on AI, Responsible Practices for Synthetic Media</a><br>
            3) <a href="https://www.gen-ai.witness.org/" rel="noopener" target="_blank">WITNESS, Prepare, Don’t Panic (deepfake field guide)</a></p>
            </div>
            </div>
            </details>

            <details><summary> <i></i>Malicious AI Misuse</summary>

            <div>
            <div><img alt="A luminous blue security shield icon being breached by aggressive red data streams, representing a cybersecurity attack." loading="lazy" src="https://d1qywhc7l90rsa.cloudfront.net/accounts/165495/images/AI_Misuse.png"></div>

            <p>AI lowers the bar for creating malicious content. This malicious content includes phishing emails, social engineering scripts, and malware. The AI models themselves are at risk through attacks like <a href="https://en.wikipedia.org/wiki/Prompt_injection" target="_blank">prompt injections</a>.</p>

            <p><strong>Example:</strong> A tailored phishing email generated in seconds steals credentials and compromises a school&#x27;s network.</p>


            <table>
            	<thead>
            		<tr>
            			<th>Factor</th>
            			<th>Assessment</th>
            		</tr>
            	</thead>
            	<tbody>
            		<tr>
            			<td><span><i></i><strong>Severity</strong></span></td>
            			<td>High. A compromise can expose sensitive data or disrupt operations.</td>
            		</tr>
            		<tr>
            			<td><span><i></i><strong>Exposure</strong></span></td>
            			<td>Broad. Any connected user or system can be a target.</td>
            		</tr>
            		<tr>
            			<td><span><i></i><strong>Plausibility</strong></span></td>
            			<td>High. Attack methods and tools are publicly known.</td>
            		</tr>
            		<tr>
            			<td><span><i></i><strong>Prevalence</strong></span></td>
            			<td>Rising. Security teams are tracking many new AI-specific vulnerabilities.</td>
            		</tr>
            		<tr>
            			<td><span><i></i><strong>Mitigation</strong></span></td>
            			<td>
            			<p><strong>Systemic &amp; Institutional:</strong> Organizations should use threat modeling, strict input filtering, and secure design principles to protect their systems.</p>

            			<p><strong>Individual &amp; Classroom Actions:</strong></p>

            			<ul>
            				<li>Be skeptical. Treat links, code, and attachments with caution.</li>
            				<li>Protect your data. Never give personal credentials to a chatbot.</li>
            				<li>Report threats. Flag malicious outputs to help developers improve safety filters.</li>
            			</ul>
            			</td>
            		</tr>
            	</tbody>
            </table>

            <div>
            <h4><i></i>Quick Activity</h4>

            <p>I dunno. Pay careful attention to your emails.</p>
            </div>

            <div>
            <h5><i></i>Dive Deeper</h5>

            <p>1) <a href="https://csrc.nist.gov/pubs/ai/100/2/e2025/final" rel="noopener" target="_blank">NIST, Adversarial ML Taxonomy (AI 100-2)</a><br>
            2) Contact the Author to suggest some more high quality resources</p>
            </div>
            </div>
            </details>


            <details><summary> <i></i> Data Security, Privacy &amp; Rights </summary>

            <div>
            <div><img alt="Tree of data protected from data harvesting monsters by a glowing shield" loading="lazy" src="https://d1qywhc7l90rsa.cloudfront.net/accounts/165495/images/data_security.png"></div>

            <p>AI applications collect and store personal data and may do so in ways users do not expect. Weak controls can lead to data breaches, re-identification of anonymous data, and illegal use. AI applications can be designed to figure things out about you; even if you are not giving them personal data, your interactions could be analyzed to create a decent profile of you.</p>

            <p><strong>Example:</strong> <a href="https://www.bbc.com/news/articles/cdrkmk00jy0o" target="_blank">Private chats in the chatbot Grok were indexed by Google </a>and became searchable by anyone.  <a href="https://techcrunch.com/2025/07/31/your-public-chatgpt-queries-are-getting-indexed-by-google-and-other-search-engines/" target="_blank">ChatGPT had a similar problem</a>.</p>


            <table>
            	<thead>
            		<tr>
            			<th>Factor</th>
            			<th>Assessment</th>
            		</tr>
            	</thead>
            	<tbody>
            		<tr>
            			<td><span><i></i><strong>Severity</strong></span></td>
            			<td>High. Breaches of sensitive data cause significant harm and legal risk.</td>
            		</tr>
            		<tr>
            			<td><span><i></i><strong>Exposure</strong></span></td>
            			<td>Large. Many users and devices handle personal data.</td>
            		</tr>
            		<tr>
            			<td><span><i></i><strong>Plausibility</strong></span></td>
            			<td>High. Standard workflows often involve sharing data with cloud tools.</td>
            		</tr>
            		<tr>
            			<td><span><i></i><strong>Prevalence</strong></span></td>
            			<td>Common. Privacy incidents happen all the time.</td>
            		</tr>
            		<tr>
            			<td><span><i></i><strong>Mitigation</strong></span></td>
            			<td>
            			<p><strong>Systemic &amp; Institutional:</strong> Institutions must practice data minimization, conduct privacy assessments, and comply with laws like FERPA and GDPR.</p>

            			<p><strong>Individual &amp; Classroom Actions:</strong></p>

            			<ul>
            				<li>Do not share secrets. Avoid pasting sensitive personal or financial information into public AI tools.</li>
            				<li>Check the settings. Review an AI&#x27;s privacy policy and opt out of data training when possible.</li>
            				<li>Use fake data. Use anonymized or hypothetical information when experimenting with new tools.</li>
            			</ul>
            			</td>
            		</tr>
            	</tbody>
            </table>

            <div>
            <h4><i></i>Quick Activity</h4>

            <p>Actually read the privacy policy of an AI company to find out what they are doing with your data.</p>
            </div>

            <div>
            <h5><i></i>Dive Deeper</h5>

            <p>1) Let me know if you have suggestions for good quality resources to go here.</p>
            </div>
            </div>
            </details>


            <details><summary> <i></i> Bias, Fairness &amp; Inclusion </summary>

            <div>
            <div><img alt="A robotic hand selecting only one color of stylized human icons from a diverse crowd, illustrating algorithmic bias." loading="lazy" src="https://d1qywhc7l90rsa.cloudfront.net/accounts/165495/images/BiasFairnessInclusion.png"></div>

            <p>AI models can adopt and amplify human biases found in their training data. This affects everything from grading tools and hiring filters to how people are portrayed.</p>

            <p><strong>Example:</strong> An image generator returns stereotyped pictures of &quot;scientists&quot; and under-represents women and people of color.</p>


            <table>
            	<thead>
            		<tr>
            			<th>Factor</th>
            			<th>Assessment</th>
            		</tr>
            	</thead>
            	<tbody>
            		<tr>
            			<td><span><i></i><strong>Severity</strong></span></td>
            			<td>Medium to high. Systematic unfairness hurts opportunities and dignity.</td>
            		</tr>
            		<tr>
            			<td><span><i></i><strong>Exposure</strong></span></td>
            			<td>Broad. Popular models are used everywhere for many different tasks.</td>
            		</tr>
            		<tr>
            			<td><span><i></i><strong>Plausibility</strong></span></td>
            			<td>High. Bias in AI models is a well-documented problem.</td>
            		</tr>
            		<tr>
            			<td><span><i></i><strong>Prevalence</strong></span></td>
            			<td>Frequent. This is especially true in general-purpose models.</td>
            		</tr>
            		<tr>
            			<td><span><i></i><strong>Mitigation</strong></span></td>
            			<td>
            			<p><strong>Systemic &amp; Institutional:</strong> Companies should use representative data, test for bias, and design inclusive and accessible products with human oversight.</p>

            			<p><strong>Individual &amp; Classroom Actions:</strong></p>

            			<ul>
            				<li>Test for bias. Actively use prompts with diverse identities and contexts to see what the AI produces.</li>
            				<li>Report it. Use feedback features to report biased outputs to developers.</li>
            				<li>Prompt better. Learn to write prompts that ask for inclusive and counter-stereotypical results.</li>
            			</ul>
            			</td>
            		</tr>
            	</tbody>
            </table>

            <div>
            <h4><i></i>Quick Activity</h4>

            <p>Prompt an image model with role labels like “CEO” or “nurse.” Tally the outputs and discuss the stereotypes you see. Then try to fix them with better prompts.</p>
            </div>

            <div>
            <h5><i></i>Dive Deeper</h5>

            <p>1) <a href="https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.1270.pdf" rel="noopener" target="_blank">NIST, SP 1270: Managing Bias in AI</a><br>
            2) Let me know if you have suggestions for good quality resources to go here.</p>
            </div>
            </div>
            </details>


            <details><summary> <i></i> Accountability, Transparency &amp; Redress </summary>

            <div>
            <div><img alt="Policy document with a magnifying glass symbolizing AI transparency, auditability, and appeals" loading="lazy" src="https://d1qywhc7l90rsa.cloudfront.net/accounts/165495/images/accountability.png"></div>

            <p>When an AI makes a decision, how do we know how it made that decision (transparency), who is responsible for the consequences of the decision (accountability), and how will those impacted by the decision be compensated for harms (redress).</p>

            <p><strong>Examples:</strong> An instructor uses an AI to flag plagiarism and punishes a student based solely on the AI&#x27;s decision. Who is accountable for this decision? A student uses AI to write a report and that report refers to papers that do not exist. Who is accountable for this mistake? An HR manager uses AI to determine a shortlist for interviews. How can he/she know how the AI came up with shortlist?</p>


            <table>
            	<thead>
            		<tr>
            			<th>Factor</th>
            			<th>Assessment</th>
            		</tr>
            	</thead>
            	<tbody>
            		<tr>
            			<td><span><i></i><strong>Severity</strong></span></td>
            			<td>Medium to high. Opaque decisions can wrongly penalize people.</td>
            		</tr>
            		<tr>
            			<td><span><i></i><strong>Exposure</strong></span></td>
            			<td>Broad. This affects anyone evaluated or served by an AI system.</td>
            		</tr>
            		<tr>
            			<td><span><i></i><strong>Plausibility</strong></span></td>
            			<td>High. Good documentation is often missing when new tech is adopted quickly.</td>
            		</tr>
            		<tr>
            			<td><span><i></i><strong>Prevalence</strong></span></td>
            			<td>Mixed. Transparency is improving but remains uneven. Accountability and redress are evolving.</td>
            		</tr>
            		<tr>
            			<td><span><i></i><strong>Mitigation</strong></span></td>
            			<td>
            			<p><strong>Organizations:</strong> publish model cards, log decisions, and require human-in-the-loop for impactful calls.</p>

            			<p><strong>Individuals:</strong></p>

            			<ul>
            				<li>Be the human-in-the-loop when possible.</li>
            				<li>Advocate for clear policies and appeal processes.</li>
            				<li>Keep records</li>
            			</ul>
            			</td>
            		</tr>
            	</tbody>
            </table>

            <div>
            <h4><i></i>Quick Activity</h4>

            <p>Transparency: Read the model card of an AI tool you use. Explain its purpose, data sources, and limitations.</p>

            <p>Accountability:</p>
            </div>

            <div>
            <h5><i></i>Dive Deeper</h5>

            <p>1) Recommend some links to me</p>
            </div>
            </div>
            </details>
            </div>
            </div>



                </div>
        </section>
        <section class="gai-box">
            <h2 class="gai-h-with-icon"><i aria-hidden="true" class="fas fa-compass"></i> Continue exploring</h2>
            <ul>
                <li><a href="ai-risks-and-ethical-considerations-ai-risks-and-ethical-impacts-part-2.html">AI Risks and Ethical Impacts Part 2</a></li>
                <li><a href="ai-risks-and-ethical-considerations-other-resources.html">Other Resources</a></li>
            </ul>
        </section>

        </div>
    </main>
</div>
</body>
</html>
